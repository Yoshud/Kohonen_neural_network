{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adam Cisak i Adama Jędrzejowski</h2>\n",
    "<h1>Notatnik z kodem sieci i wczytywania danych </h1>\n",
    "( całym kodem któy był pisany poza Jupyterem )\n",
    "<h4>Opis wyników jest w notatniku wyniki.html (ze względu na wygodę przeglądania), od którego zalecamy zacząć</h4>\n",
    "<h5>Projekt dostępny również pod adresem: <a href = https://www.github.com/Yoshud/Kohonen_neural_network>www.github.com/Yoshud/Kohonen_neural_network</a></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>main.py </h2>\n",
    "<h4>(plik zawierający sposób wczytywania danych)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_LABELS = {\n",
    "    \"age\": 0,\n",
    "    \"gender\": 1,\n",
    "    \"tb\": 2,\n",
    "    \"db\": 3,\n",
    "    \"alkphos\": 4,\n",
    "    \"sgpt\": 5,\n",
    "    \"sgot\": 6,\n",
    "    \"tp\": 7,\n",
    "    \"alp\": 8,\n",
    "    \"ag\": 9,\n",
    "    \"class\": 10\n",
    "}\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    with open(\"data.csv\", newline='') as data_csv:\n",
    "        data_read = csv.reader(data_csv, delimiter=',', quotechar='|')\n",
    "        a = np.array(list(list(data_read)))\n",
    "        a_pd = pd.read_csv(\"data.csv\", sep=\",\")\n",
    "        a_pd = parser(a_pd)\n",
    "        return a_pd.T\n",
    "\n",
    "\n",
    "def parser(data):\n",
    "    return pd.DataFrame([parse_gender(col) if it == DATA_LABELS[\"gender\"] else parse_class(col) if it == DATA_LABELS[\n",
    "        \"class\"] else normalize(col) for it, col in enumerate(np.array(data.T))])\n",
    "\n",
    "\n",
    "def normalize(row):\n",
    "    max_v = max(row)\n",
    "    return list(map(lambda el, max_v=max_v: el / max_v, row))\n",
    "\n",
    "\n",
    "def parse_gender(row):\n",
    "    return list(map(lambda el: 0 if el == \"Female\" else 1, row))\n",
    "\n",
    "\n",
    "def parse_class(row):\n",
    "    return list(map(lambda el: abs(el - 2), row))  # '1' - zywy, '0' - martwy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>neural_network.py</h2>\n",
    "<h4>(Definicje sieci, SOM i MLP, gdyż początkowo projekt miał być dość uniwersalny, ale odeszliśmy od tego pomysłu)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import layer #plik widoczny niżej\n",
    "from functools import reduce\n",
    "from abc import ABC, abstractmethod #funkcje abstrakcyjne w Pythonie\n",
    "\n",
    "\n",
    "class NeuralNetwork(ABC):\n",
    "    # @abstractmethod\n",
    "\n",
    "    def set_data(self, treining_set, test_set):\n",
    "        self.treining_set = treining_set\n",
    "        self.test_set = test_set\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_structure(self, structure, attributes_in_data):\n",
    "        pass\n",
    "\n",
    "\n",
    "# sieć typu MLP bez algorytmu uczenia\n",
    "class MultiLayerNetwork(NeuralNetwork):\n",
    "    def __init__(self, structure, attributes_in_data):\n",
    "        self.set_structure(structure, attributes_in_data)\n",
    "\n",
    "    def set_structure(self, structure, attributes_in_data):\n",
    "        self.layers = []\n",
    "\n",
    "        def add_layer(input, output, self=self):\n",
    "            self.layers.append(layer.Layer(input, output, expit))\n",
    "            return output\n",
    "\n",
    "        reduce(add_layer, structure, attributes_in_data)\n",
    "\n",
    "    def compute(self, x):\n",
    "        return reduce(lambda returned, layer: layer.compute(returned), self.layers, x)\n",
    "\n",
    "#Sieć typu SOM, ma zaimplementowany algorytm uczenia jej używający\n",
    "class Kohonen2DNetwork(NeuralNetwork):\n",
    "    def __init__(self, structure, attributes_in_data):\n",
    "        self.set_structure(structure, attributes_in_data)\n",
    "\n",
    "    def set_structure(self, structure, attributes_in_data):\n",
    "        self.layers = layer.Kohonen2DLayer(attributes_in_data, structure)\n",
    "\n",
    "#\n",
    "#     def compute(self, x):\n",
    "#         return reduce(lambda returned, layer: layer.compute(returned), self.layers, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>layer.py</h2>\n",
    "<h4>(Definicje warstw sieci. Tu znajduje się dość znacząca część kodu sieci SOM)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cupy as cp #biblioteka działająca jak numpy, tylko że wykonująca operacje na GPU na rdzenach CUDA\n",
    "import numpy as np #najpopularniejsza pythonowa biblioteka do operacji matematycznych głównie macierzowych\n",
    "from math import floor, sqrt\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# klasa przydatna na wypadek chęci tworzenia sieci MLP\n",
    "class Layer:\n",
    "    def __init__(self, number_of_input, number_of_output, activation_fun):\n",
    "        self.W = cp.random.rand(number_of_input, number_of_output)\n",
    "        self.activation_fun = activation_fun\n",
    "\n",
    "    def compute(self, x):\n",
    "        return self.activation_fun(cp.dot(x, self.W))\n",
    "\n",
    "    def set_structure(self, number_of_input, number_of_output):\n",
    "        self.W = cp.random.rand(number_of_input, number_of_output)\n",
    "\n",
    "\n",
    "class Kohonen2DLayer:\n",
    "    def __init__(self, number_of_input, number_of_output):\n",
    "        # self.distance_dependency_fun = distance_dependency_fun\n",
    "        self.set_structure(number_of_input, number_of_output)\n",
    "\n",
    "    def set_structure(self, number_of_input, number_of_output):\n",
    "        self.W = cp.random.rand(number_of_input, number_of_output)\n",
    "        positions = list([])\n",
    "        floor_sqrt_output = int(floor(sqrt(number_of_output)))\n",
    "        for number_x in range(0, floor_sqrt_output):\n",
    "            for number_y in range(0, floor_sqrt_output):\n",
    "                positions.append([number_x, number_y])\n",
    "        for number_y in range(0, number_of_output - floor_sqrt_output ** 2):\n",
    "            positions.append([floor_sqrt_output, number_y])\n",
    "        self.positions = cp.array(positions)\n",
    "\n",
    "    # licz produkt macierzowy, właściwie zbędna funkcja\n",
    "    def compute(self, x):\n",
    "        return cp.dot(x, self.W)\n",
    "\n",
    "    # wyszukuje BMU - neuronu najsilniej odpowiadającego. Korzysta z określania odległości jako kąty\n",
    "    def find_best_neuron_by_dot_product(self, x):\n",
    "        neurons_output = cp.dot(x, self.W).transpose()\n",
    "        output_max = neurons_output[0]\n",
    "        it_max = 0\n",
    "        for it, product in enumerate(neurons_output):\n",
    "            if abs(product) > abs(output_max):\n",
    "                it_max = it\n",
    "                output_max = product\n",
    "        return it_max\n",
    "\n",
    "    # wyszukuje BMU - neuronu najsilniej odpowiadającego\n",
    "    def find_best_neuron_by_cartesian_distance(self, x):\n",
    "        W = self.W.transpose()\n",
    "        # funkcja ta nie jest dokładnie dystansem kartezjanskim ale zachowuje jego monotoniczność\n",
    "        return reduce(lambda max, pretender: pretender if pretender[0] > max[0] else max,\n",
    "                      map(lambda w, it, x=x: (abs(cp.sum(w - x)), it), W, range(len(W))))[1]\n",
    "\n",
    "    # zakłada prostokątną architekture sieci, i liczy odległość między neuronami w \"przestrzeni neuronów\"\n",
    "    def cartesian_distance_between_neurons(self, it_1, it_2):  # dla sieci o topoligi prostokątnej\n",
    "        dist_x = (self.positions[it_1][0] - self.positions[it_2][0])\n",
    "        dist_y = (self.positions[it_1][1] - self.positions[it_2][1])\n",
    "        return sqrt(dist_x ** 2 + dist_y ** 2)\n",
    "\n",
    "    def distance_between_neurons_as_vectors(self, it_1, it_2):\n",
    "        W_1 = self.W[it_1]\n",
    "        W_2 = self.W[it_2]\n",
    "        # return sqrt(reduce(lambda sum, w: sum + w ** 2, W_1 - W_2))\n",
    "        return sqrt(cp.sum((W_1 - W_2) ** 2))\n",
    "\n",
    "    def number_of_neurons(self):\n",
    "        return len(self.W.transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>kohonen_network_learning.py</h2>\n",
    "<h4>(Definicja algorytmu uczenia sieci typu SOM. Na tym etapie projektu kod jest dość specyficzny, ale do etapu 3 będziemy chcieli go uogólnić pozwalająć na modyfikacje zachowania algorytmu z zewnątrz, a nie poprzez zmiane wewnątrz implementacji)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm  # potrzebne do prostej i szybkiej implementacji funkcji Gaussa\n",
    "import copy  # używane do głębokich kopii sieci\n",
    "\n",
    "\n",
    "# funktor pozwalający na ustawienie i potem zawsze zwracanie wartości funkcji Gaussa dla nastaw i danego punktu,\n",
    "# zawsze zwraca wartość z 1 w mean - służy do liczenia wpływu odległości\n",
    "class Gauss:\n",
    "    def __init__(self, mean, std_dev):\n",
    "        self.rv = norm(loc=mean, scale=std_dev)\n",
    "        self.max_rv = self.rv.pdf(mean)\n",
    "\n",
    "    def val(self, x):\n",
    "        return self.rv.pdf(x) / self.max_rv\n",
    "\n",
    "\n",
    "# sieć SOM z funkcją Gaussa jako funkcją odległości i ustaloną funkcją dla learning rate, w późniejszych etapach projektu\n",
    "# ulegnie refaktoryzacji dla większej elastyczności (możliwość wyboru poszczególnych funkcji i ich parametrów)\n",
    "class KohonenNetworkGauss_nbh:\n",
    "    def __init__(self, data, network, dist_0=20, alpha_0=0.1, if_by_dot_product=False):\n",
    "        self.data = data\n",
    "        self.network = network\n",
    "        self.by_dot_product = if_by_dot_product\n",
    "        self.dist_0 = dist_0\n",
    "        self.a_0 = alpha_0\n",
    "\n",
    "    # funcja zwracająca indeksy danych w losowej kolejności by móc później \"karmić\" sieć danymi w losowej kolejności\n",
    "    def randomize_data_indexes(self):\n",
    "        one = list(range(0, len(self.data)))\n",
    "        second = list(np.random.rand(len(self.data)))\n",
    "        x = np.concatenate((np.array([one]), np.array([second])), axis=0)\n",
    "        lista = []\n",
    "        for el in x.transpose():\n",
    "            lista.append((el[0], el[1]))\n",
    "        lista.sort(key=lambda x: x[1])\n",
    "        return list(map(lambda el: int(el[0]), lista))\n",
    "\n",
    "    # główna część - algorytm uczenia się\n",
    "    # BMU - indeks najsilniej odpowiadającego neuronu\n",
    "    def learning(self, number_of_epochs):\n",
    "\n",
    "        # służy do zapisywania stanów sieci w kolejnych epokach do późniejszej obserwacji\n",
    "        remember_network = [copy.deepcopy(self.network), ]\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            for index in self.randomize_data_indexes():\n",
    "                x = self.data[index].reshape(1, len(self.data[0]))\n",
    "                s = epoch / number_of_epochs\n",
    "                if (self.by_dot_product):\n",
    "                    BMU = self.network.layers.find_best_neuron_by_dot_product(x)\n",
    "                else:\n",
    "                    BMU = self.network.layers.find_best_neuron_by_cartesian_distance(x)\n",
    "                # print(BMU)\n",
    "                self.nbh_fun(x, BMU, s, self.dist_0)\n",
    "            print(\"epoch: \" + str(epoch))\n",
    "            print(self.network.layers.W.transpose()[0])\n",
    "            remember_network.append(copy.deepcopy(self.network))\n",
    "        return remember_network\n",
    "\n",
    "    # funkcja malejąca do learning rate\n",
    "    def alpha_fun(self, s, a0=1., t=2.0):\n",
    "        if (s != 1):\n",
    "            return a0 * np.exp(t - t / (1 - s))\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    # funkcja ustalająca zmniejszający się rozmiar zasięgu funkcji sąsiedstwa\n",
    "    def deacresing_dist_fun(self, dist0, s, t=1.2):\n",
    "        return self.alpha_fun(s, dist0, t)\n",
    "\n",
    "    # nazwa jest niedokładna. Jest to funkcja sąsiedstwa, oraz algorytm uczenia neuronu na jej podstawie:\n",
    "    # x - wektor wejściowy\n",
    "    # BMU - indeks neuronu najmocniej odpowiadającego\n",
    "    # s - zmienna informująca o tym ile zostało do zakończenia epok\n",
    "    # dist_0 - zasięg w \"neuronach\" początkowy - zasięg definiowany jako odchylenie standardowe funkcji Gaussa\n",
    "    def nbh_fun(self, x, BMU, s, dist_0):\n",
    "        layer = self.network.layers\n",
    "        W = layer.W.transpose()\n",
    "        dist_0_act = self.deacresing_dist_fun(dist_0, s)\n",
    "        gauss = Gauss(0., dist_0_act)  # uzywamy zwyklego Gaussa1D do policzenia \"wzmocnienia\" w funkcji odleglosci\n",
    "        for v in range(layer.number_of_neurons()):\n",
    "            dist = layer.cartesian_distance_between_neurons(BMU, v)\n",
    "            k = gauss.val(dist)\n",
    "            W[v] = W[v] + (self.alpha_fun(s, self.a_0, 0.7) * k * (x[0] - W[v]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
